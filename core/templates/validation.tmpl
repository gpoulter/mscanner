<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">

#* Validation output page

$stats -- FeatureScores statistics
$linkpath -- None, or where to link for JS/CSS instead of #include
$timestamp -- time.time() for start of the operation
$p -- PerformanceStats instance (with p.tuned)
$notfound_pmids -- List of invalid PubMed IDs
*#

#from mscanner.configuration import rc
## Store tuned performance stats in t
#set $t = $p.tuned

<html lang="en">

<head>

  <title>MScanner validation results: $rc.dataset</title>
  <meta http-equiv="content-type" content="text/html; charset=UTF-8">

  #if $getVar("linkpath", None)
  <script type="text/javascript" src="$linkpath/hider.js"></script>
  <link rel="stylesheet" type="text/css" href="$linkpath/style.css">
  #else

  <script type="text/javascript">
    #include raw str($rc.templates/"hider.js")
  </script>

  <style type="text/css">
    #include raw str($rc.templates/"style.css")
  </style>
  #end if
  
  <script type="text/javascript">
  window.onload = function() { make_toggles(); }
  </script>
  
</head>

#def help(name)
<td>
  #if $name is not None
  <button class="toggle" id="toggle_$name">?</button>
  #end if
</td>
#end def

#def hr(name, value)
<tr id="$name" class="help">
  <td colspan="3">
    $value
  </td>
</tr>
#end def

<body>

<div id="header">
 <h1>MScanner validation results: $rc.dataset</h1>
</div>

<div id="body">

<div class="narrow">

<h2>Global information</h2>

<table class="results">

  <colgroup>
    <col class="label">
    <col class="value">
    <col class="help">
  </colgroup>

  <thead>
    <tr>
      <th>Name</th>
      <th>Value</th>
      <th>Help</th>
    </tr>
  </thead>

  <tbody>

  <tr>
    <td>Timestamp</td>
    #import time
    <td>$time.strftime("%Y/%m/%d %H:%M:%S GMT", $time.gmtime($timestamp))</td>
    $help("h_timestamp")
  </tr>
  $hr("h_timestamp", """Date and time at which the query was submitted.""")

  <tr>
    <td>Feature scores</td>
    <td><a href="$rc.report_term_scores">$rc.report_term_scores</a></td>
    $help("h_csv")
  </tr>
  $hr("h_csv", """CSV spreadsheet with the support scores for the
  features (MeSH terms and ISSNs).  The number of positive and
  negative occurrences and pseudocount are also included, as these
  go into calculating the support score.""")

  <tr>
    <td>Positive PMIDs</td>
    <td><a href="$rc.report_positives">$rc.report_positives</a></td>
    $help("h_positives")
  </tr>
  $hr("h_positives", """PubMed IDs for positive training documents,
  and their validation scores.  The file can be divided into the 
  original N folds used for validation.""")

  <tr class="endsection">
    <td>Negative PMIDs</td>
    <td><a href="$rc.report_negatives">$rc.report_negatives</a></td>
    $help("h_negatives")
  </tr>
  $hr("h_negatives", """PubMed IDs for negative training documents
  and their validation scores.  The file can be divided into the 
  original N folds used for validation.""")

  <tr>
    <td>Number of folds</td>
    <td>$rc.nfolds</td>
    $help("h_nfolds")
  </tr>
  $hr("h_nfolds", """Number of partitions to split the positive
  and negative data into.  Repeatedly, one partition from each
  has its scores calculated, using the feature scores trained from the
  remaining partitions.""")

  #if $stats.offset != 0
  <tr>
    <td>Offset</td>
    <td>$stats.offset</td>
    $help("h_offset")
  </tr>
  $hr("h_offset", """This value includes an offset to account for absent
  features, and a prior, and is added to each citation score. It just shifts
  the score, so does not affect performance. It's only use is to make the score
  represent the natural logarithm of the ratio of posterior probabilities for
  the article being relevant versus non-relevant, given its features.""")
  #end if
  
  <tr class="endsection">
    <td>F-Measure Alpha</td>
    <td>$rc.alpha</td>
    $help("h_alpha")
  </tr>
  $hr("h_alpha", """The weight of precision in the &alpha;-weighted
  F-Measure (which is maximised to choose the threshold).
  &alpha;=0.5 weights precision and recall equally, &alpha; closer
  to 1.0 trades recall for increased precision.""")

  <tr>
    <td>Averaged Precision</td>
    <td>#echo "%.5f" % $p.AvPrec #</td>
    $help("h_avprec")
  </tr>
  $hr("h_avprec", """Average precision, where the precision is evaluated at
  each point where a relevant citation is retrieved.  This is the most
  important summary metric.""")
  
  <tr class="endsection">
    <td>Area under PR curve</td>
    <td>#echo "%.5f" % $p.PR_area #</td>
    $help("h_prcurve")
  </tr>
  $hr("h_prcurve", """Area under the Precision-Recall (PR) curve, using
  trapezoidal rule over each distinct threshold. This is almost the same
  numerical value as the average precision.""")
  
  <tr>
    <td>Area under ROC curve A(z)</td>
    <td>#echo "%.5f" % $p.W #</td>
    $help("h_roc")
  </tr>
  $hr("h_roc", """Area under ROC is a global data-size-independent
  measure of performance.  Worst case is 0.5 (diagonal line: true positives 
  increase at same rate as false positives), and best case is 1.0.""")

  <tr class="endsection">
    <td>Standard Error of A(z)</td>
    <td>#echo "%.5f" % $p.W_stderr #</td>
    $help("h_roc_stderr")
  </tr>
  $hr("h_roc_stderr", """Area is equivalent to the Wilcoxon statistic W, for which 
  we use the method of Hanley1982 to calculate the standard error.""")

  <tr>
    <td>Break-Even <span class="footnote">(where precision=recall)</span></td>
    <td>#echo "%.5f" % $p.breakeven #</td>
    $help("h_break_even")
  </tr>
  $hr("h_break_even", """Shared value at the point where Recall = Precision =
  F1-measure. Typically the F1-Measure at break-even is slightly lower than
  the maximum F1-Measure.""")

  <tr>
    <td>Score threshold</td>
    <td>#echo "%.2f" % $p.threshold #</td>
    $help("h_threshold")
  </tr>
  $hr("h_threshold", """The score threshold which maximises
  &alpha;-weighted F-Measure.""")
  
  </tbody>

  </table>

  #include str($rc.templates/"features.tmpl")

  <h2>Classifier Confusion Matrix
  <button class="toggle" id="toggle_h_cmatrix">?</button>
  </h2>

  <div class="help" id="h_cmatrix">
  <p>
  The columns of the confusion matrix are actual categories, and
  the rows are predicted categories.  Hover the mouse over each of the squares
  for a full description of the quantity, and the formula for calculating 
  it.
  </p>
  </div>

  <table id="matrix">
    <thead>
      <tr>
      <th rowspan="2" colspan="2"></th>
      <th colspan="2">Actual</th>
      <th rowspan="2">Totals</th>
      <th rowspan="2">Rates</th>
      </tr>
      <tr>
        <th>Relevant</th>
        <th>Irrelevant</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <th rowspan="2">Predicted</th>
        <th>Relevant'</th>
        <td class="tp" title="True Positives (hits)">TP=$t.TP</td>  
        <td class="fp" title="False Positives (false alarms / Type I errors)">FP=$t.FP</td>
        <td title="Positive predictions = TP+FP">P'=#echo $t.TP+$t.FP#</td>
        <td title="Positive Predictive Value = TP/(TP+FP)">PPV=#echo "%.2f"%$t.PPV#</td>
      </tr>
      <tr>
        <th>Irrelevant'</th>
        <td class="fn" title="False Negatives (misses / Type II errors)">FN=$t.FN</td>
        <td class="tn" title="True Negatives (correct rejections)">TN=$t.TN</td>
        <td title="Negative predictions = TN+FN">N'=#echo $t.TN+$t.FN#</td>
        <td title="Negative Predictive Value = TN/(TN+FN)">NPV=#echo "%.5f"%t.NPV#</td>
      </tr>
      <tr>
        <th colspan="2">Totals</th>
        <td title="Positives = TP+FN">P=$t.P</td>
        <td title="Negatives = TN+FP">N=$t.N</td>
        <td title="Size of data = TP+TN+FP+FN">$t.A</td>
        <td title="Prevalence = (TP+FN)/(TP+TN+FP+FN)">Prev=#echo "%.5f"%(float($t.P)/$t.A)#</td>
      </tr>
      <tr>
        <th colspan="2">Rates</th>
        <td title="True Positive Rate (OR hit rate OR recall OR sensitivity) = TP/(TP+FN)">TPR=#echo "%.2f"%$t.TPR#</td>  
        <td title="False Positive Rate = FP/(TN+FP)">FPR=#echo "%.5f"%$t.FPR#</td>
        <td title="This square is intentionally left blank"></td>
        <td title="Accuracy = (TP+TN)/(TP+TN+FP+FN)">Acc=#echo "%.5f"%t.accuracy#</td>
      </tr>
    </tbody>
  </table>
  
   
  <h2>Common Performance Measures
  <button class="toggle" id="toggle_h_performance">?</button>
  </h2>

  <div class="help" id="h_performance">
    <p>These are performance measures which are commonly quoted.  All of
    these measures are derived from the confusion matrix.</p>
    
    <p>For precision, recall and F<sub>1</sub> measure, we have additionally
    calculated their value within each of the cross validation folds
    (the main value comes from aggregating the scores across the folds).</p>
  
    <dl>
      <dt>Recall (True Positive Rate / Sensitivity)</dt>
      <dd>
        Proportion of positives which were correctly predicted to be positive.
      </dd>

      <dt>Precision (Positive Predictive Value)</dt>
      <dd>
        Proportion of predicted positives which are true positives.
      </dd>

      <dt>F<sub>1</sub>-Measure (&alpha;=0.5)</dt>
      <dd>
        Harmonic mean of recall and precision at the threshold
        corresponding to the maximum &alpha;-weighted F-Measure.
      </dd>

      <dt>F-Measure (&alpha;=$p.alpha)</dt>
      <dd>
        Decision threshold is chosen to maximise F-Measure. 0 &lt;=
        &alpha; &lt;= 1 controls the weight of precision.  When
        &alpha;=0.5, <em>F=F<sub>1</sub></em>.
      </dd>

      <dt>Maximum F1-Measure</dt>
      <dd>
        Maximum possible value of the F<sub>1</sub>-Measure (which
        is achieved if &alpha;=0.5).
      </dd>

      <!-- *************** -->
      <dt>Prevalence</dt>
      <dd>
        Proportion of training data which was positive
      </dd>
      
      <dt>Accuracy</dt>
      <dd>
        Proportion of correct predictions
      </dd>

      <dt>Error Rate (1-Accuracy)</dt>
      <dd>
        Proportion of incorrect predictions
      </dd>

      <!-- *************** -->
      <dt>Negative Predictive Value</dt>
      <dd>
        Proportion of predicted negatives which are true negatives.
      </dd>

      <dt>Specificity (True Negative Rate)</dt>
      <dd>
        Proportion of negatives which were correctly predicted to be negative.
      </dd>

      <dt>False Positive Rate (1-Specificity)</dt>
      <dd>
        Proportion of negatives which were incorrectly predicted to be positive.
      </dd>
      
      <dt>False Discovery Rate (1-Precision)</dt>
      <dd>
        Proportion of predicted positives which are false positives.
      </dd>

      <dt>FP/TP Ratio</dt>
      <dd>
        Ratio of false positives to true positives.  Related to precision: 
        if TP/FP is <i>x</i>, then precision is <i>1/(x+1)</i>.
      </dd>

      <dt>Enrichment</dt>
      <dd>
        Precision over prevalence.  This is is how much better this
        classifier's precision is over a classifier which calls
        everything positive.
      </dd>
      
  </dl>
  </div>

  <table class="results">
    <colgroup>
      <col class="longlabel">
      <col class="value">
    </colgroup>
    <tr>
      <td><b>Recall (Sensitivity/TPR)</b> 
      <span class="footnote">&rho;=TP/(TP+FN)</span></td>
      <td>#echo "%.5f" % $t.recall #
      #echo " (%.5f to %.5f)" % $perfrange.recall #
      </td>
    </tr>
    <tr>
      <td><b>Precision (PPV)</b> 
      <span class="footnote">&pi;=TP/(TP+FP)</span></td>
      <td>#echo "%.5f" % $t.PPV #
      #echo " (%.5f to %.5f)" % $perfrange.precision #
      </td>
    </tr>
    <tr>
      <td><b>F<sub>1</sub>-Measure (&alpha;=0.5)</b> 
      <span class="footnote">(2*&rho;*&pi;/(&rho;+&pi;))</span></td>
      <td>#echo "%.5f" % $t.fmeasure #
      #echo " (%.5f to %.5f)" % $perfrange.fmeasure #
      </td>
    </tr>
    <tr>
      <td><b>F-Measure (&alpha;=$p.alpha)</b> 
      <span class="footnote">(1/(&alpha;/&pi;+(1-&alpha;)/&rho;))</span></td>
      <td>#echo "%.5f" % $t.fmeasure_alpha #</td>
    </tr>
    <tr class="endsection">
      <td><b>Maximum possible F<sub>1</sub>-Measure</b></td>
      <td>#echo "%.5f" % $t.fmeasure_max #</td>
    </tr>
    <tr>
      <td><b>Prevalence</b> 
      <span class="footnote">P/(P+N)</span></td>
      <td>#echo "%.5f" % $t.prevalence #</td>
    </tr>
    <tr>
      <td><b>Accuracy</b> 
      <span class="footnote">(TP+TN)/(P+N)</span></td>
      <td>#echo "%.5f" % $t.accuracy #</td>
    </tr>
    <tr>
      <td><b>Error Rate</b> 
      <span class="footnote">(FP+FN)/(P+N)=1-Accuracy</span></td>
      <td>#echo "%.5f" % (1-$t.accuracy) #</td>
    </tr>
    <tr class="endsection">
      <td><b>False Discovery Rate (1-precision)</b></td>
      <td>#echo "%.5f" % $t.FDR #</td>
    </tr>
    <tr>
      <td><b>Specificity (TNR)</b> 
      <span class="footnote">TNR=TN/(TN+FP)=1-FPR</span></td>
      <td>#echo "%.5f" % $t.TNR #</td>
    </tr>
    <tr>
      <td><b>False Positive Rate (FPR)</b> 
      <span class="footnote">FPR=FP/(TN+FP)=1-TNR</span></td>
      <td>#echo "%.5f" % $t.FPR #</td>
    </tr>
    <tr>
      <td><b>Negative Preditive Value</b> 
      <span class="footnote">NPV=TN/(TN+FN)</span></td>
      <td>#echo "%.5f" % $t.NPV #</td>
    </tr>
    <tr>
      <td><b>FP/TP Ratio</b></td>
      <td>#echo "%.5f" % t.fp_tp_ratio #</td>
    </tr>
    <tr>
      <td><b>Enrichment</b> 
      <span class="footnote">(= precision/prevalence)</span></td>
      <td>#echo "%.5f" % $t.enrichment #</td>
    </tr>
  </table>

  <h2>Performance graphs
  <button class="toggle" id="toggle_h_graphs">?</button>
  </h2>

  <div class="help" id="h_graphs">
    <dl>
      <dt>Article Score Densities</dt>
      <dd>
        Histograms of article scores for negative and positive
        articles.  Classifier performs better when the distributions
        are well-separated. Tuned threshold is marked with a vertical
        line.
      </dd>
      <dt>Feature Score Density</dt>
      <dd>
        Histogram of feature scores.
      </dd>
      <dt>ROC Curve</dt>
      <dd>
        True Positive Rate (recall) against False Positive Rate
        (1-specificity).  The greater the area under the ROC curve,
        the better the global performance of the classifier.  In the
        worst case, true positives increase at the same rate as false
        positives, yielding a 45-degree line.
      </dd>
      <dt>Precision-Recall Curve</dt>
      <dd>
        Precision against Recall.  In good cases, the precision only
        drops off at high recall.  Tuned threshold is marked with a
        vertical line.  Worst case is a horizontal line with precision
        equal to prevalence.
      </dd>
      <dt>Variation against Threshold</dt>
      <dd>
        Precision, recall and F-measure against threshold shows how
        the threshold was optimised for &alpha;-weighted F-Measure and
        how changing the threshold would affect the recall/precision
        balance.
      </dd>
    </dl>
  </div>

  <img src="$rc.report_artscores_img" alt="Article Score Densities"/>
  
  <img src="$rc.report_featscores_img" alt="Feature Score Density"/>
  
  <img src="$rc.report_roc_img" alt="ROC Curve"/>
  
  <img src="$rc.report_prcurve_img" alt="Precision-Recall Curve"/>
  
  <img src="$rc.report_fmeasure_img" alt="Precision and Recall vs Threshold"/>

  #include str($rc.templates/"invalid.tmpl")

</div><!--narrow-->

</div><!--body-->

<div id="footer">
                                     
</div>

</body>
</html>