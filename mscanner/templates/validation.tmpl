<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">

#* Validation output page

$stats -- FeatureInfo statistics
$linkpath -- None, or where to link for JS/CSS instead of #include
$p -- PerformanceStats instance (with p.tuned)
$notfound_pmids -- List of invalid PubMed IDs
*#

#from mscanner.configuration import rc
## Store tuned performance stats in t
#set $t = $p.tuned

<html lang="en">

<head>

  <title>MScanner validation results: $rc.dataset</title>
  <meta http-equiv="content-type" content="text/html; charset=UTF-8">

  #if $getVar("linkpath", None)
  <script type="text/javascript" src="$linkpath/hider.js"></script>
  <link rel="stylesheet" type="text/css" href="$linkpath/style.css">
  #else

  <script type="text/javascript">
    #include raw str($rc.templates/"hider.js")
  </script>

  <style type="text/css">
    #include raw str($rc.templates/"style.css")
  </style>
  #end if
  
  <script type="text/javascript">
  window.onload = function() { make_toggles(); }
  </script>
  
</head>

#def help(name)
<td>
  #if $name is not None
  <button class="toggle" id="toggle_$name">?</button>
  #end if
</td>
#end def

#def hr(name, value)
<tr id="$name" class="help">
  <td colspan="3">
    $value
  </td>
</tr>
#end def

<body>

<div id="header">
 <h1>MScanner validation results: $rc.dataset</h1>
</div>

<div id="body">

<div class="narrow">

<h2>Global information</h2>

<table class="results">

  <colgroup>
    <col class="label">
    <col class="value">
    <col class="help">
  </colgroup>

  <thead>
    <tr>
      <th>Name</th>
      <th>Value</th>
      <th>Help</th>
    </tr>
  </thead>

  <tbody>

    <tr>
      <th>Timestamp</th>
      #import time
      <td>$time.strftime("%Y/%m/%d %H:%M:%S UTC", $time.gmtime($rc.timestamp))</td>
      $help("h_timestamp")
    </tr>
    $hr("h_timestamp", """Date and time at which the query was submitted.""")

    <tr>
      <th>Feature scores</th>
      <td><a href="$rc.report_term_scores">$rc.report_term_scores</a></td>
      $help("h_csv")
    </tr>
    $hr("h_csv", """CSV spreadsheet with the support scores for the
    features (MeSH terms and ISSNs).  The number of positive and
    negative occurrences and pseudocount are also included, as these
    go into calculating the support score.""")

    <tr>
      <th>Positive PMIDs</th>
      <td><a href="$rc.report_positives">$rc.report_positives</a></td>
      $help("h_positives")
    </tr>
    $hr("h_positives", """PubMed IDs for positive training documents.""")

    <tr>
      <th>Negative PMIDs</th>
      <td><a href="$rc.report_negatives">$rc.report_negatives</a></td>
      $help("h_negatives")
    </tr>
    $hr("h_negatives", """PubMed IDs for negative training documents
    (randomly selected).  Any overlap with the positive training
    documents is removed from this.""")

    <tr>
      <th>Number of folds</th>
      <td>$rc.nfolds</td>
      $help("h_nfolds")
    </tr>
    $hr("h_nfolds", """Number of partitions to split the positive
    and negative data into.  One partition of each is used for
    calculating articles scores (the feature scores having been
    trained using the remaining partitions).  If zero, leave-out-one
    validation is used, in which the feature scores are calculated
    leaving out only the article which is to be scored (this mode is
    about 5x slower than 10-fold cross validation).""")

    <tr>
      <th>F-Measure Alpha</th>
      <td>$rc.alpha</td>
      $help("h_alpha")
    </tr>
    $hr("h_alpha", """The weight of precision in the &alpha;-weighted
    F-Measure (which is maximised to choose the threshold).
    &alpha;=0.5 weights precision and recall equally, &alpha; closer
    to 1.0 trades recall for increased precision.""")

    <tr>
      <th>Area under ROC curve A(z)</th>
      <td>#echo "%.5f" % $p.W #</td>
      $help("h_roc")
    </tr>
    $hr("h_roc", """Area under ROC is a global data-size-independent
    measure of performance.  Worst case is 0.5 (diagonal line: true positives 
    increase at same rate as false positives), and best case is 1.0.""")

    <tr>
      <th>Standard Error of A(z)</th>
      <td>#echo "%.5f" % $p.W_stderr #</td>
      $help("h_roc_stderr")
    </tr>
    $hr("h_roc_stderr", """Area is equivalent to the Wilcoxon statistic W, for which 
    we use the method of Hanley1982 to calculate the standard error.""")

    <tr>
      <th>Area under PR curve</th>
      <td>#echo "%.5f" % $p.PR_area #</td>
      $help("h_prcurve")
    </tr>
    $hr("h_prcurve", """Area under the Precision-Recall (PR) curve,
    using trapezoidal rule over each distinct threshold.""")
    
    <tr>
      <th><b>Averaged Precision</b></th>
      <td>#echo "%.5f" % $p.AvPrec #</td>
      $help("h_avprec")
    </tr>
    $hr("h_avprec", """Average value of precision, where the
    precision is evaluated at each point where a relevant citation is 
    retrieved.""")
    
    <tr>
      <th>Break-Even <span class="footnote">(where precision=recall)</span></th>
      <td>#echo "%.5f" % $p.breakeven #</td>
      $help("h_break_even")
    </tr>
    $hr("h_break_even", """Shared value at the point where Recall = Precision =
    F1-measure. Typically the F1-Measure at break-even is slightly lower than
    the maximum F1-Measure.""")

    <tr>
      <th>Score threshold</th>
      <td>#echo "%.2f" % $p.threshold #</td>
      $help("h_threshold")
    </tr>
    $hr("h_threshold", """The score threshold which maximises
    &alpha;-weighted F-Measure.""")

  </table>

  #include str($rc.templates/"features.tmpl")

  <h2>Classifier Confusion Matrix
  <button class="toggle" id="toggle_h_cmatrix">?</button>
  </h2>

  <div class="help" id="h_cmatrix">
  <p>
  The columns of the confusion matrix are actual categories, and
  the rows are predicted categories.  Hover the mouse over each of the squares
  for a full description of the quantity, and the formula for calculating 
  it.
  </p>
  </div>

  <table id="matrix">
    <thead>
      <tr>
      <th rowspan="2" colspan="2"></th>
      <th colspan="2">Actual</th>
      <th rowspan="2">Totals</th>
      <th rowspan="2">Rates</th>
      </tr>
      <tr>
        <th>Relevant</th>
        <th>Irrelevant</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <th rowspan="2">Predicted</th>
        <th>Relevant'</th>
        <td class="tp" title="True Positives (hits)">TP=$t.TP</td>  
        <td class="fp" title="False Positives (false alarms / Type I errors)">FP=$t.FP</td>
        <td title="Positive predictions = TP+FP">P'=#echo $t.TP+$t.FP#</td>
        <td title="Positive Predictive Value = TP/(TP+FP)">PPV=#echo "%.2f"%$t.PPV#</td>
      </tr>
      <tr>
        <th>Irrelevant'</th>
        <td class="fn" title="False Negatives (misses / Type II errors)">FN=$t.FN</td>
        <td class="tn" title="True Negatives (correct rejections)">TN=$t.TN</td>
        <td title="Negative predictions = TN+FN">N'=#echo $t.TN+$t.FN#</td>
        <td title="Negative Predictive Value = TN/(TN+FN)">NPV=#echo "%.5f"%t.NPV#</td>
      </tr>
      <tr>
        <th colspan="2">Totals</th>
        <td title="Positives = TP+FN">P=$t.P</td>
        <td title="Negatives = TN+FP">N=$t.N</td>
        <td title="Size of data = TP+TN+FP+FN">$t.A</td>
        <td title="Prevalence = (TP+FN)/(TP+TN+FP+FN)">Prev=#echo "%.5f"%(float($t.P)/$t.A)#</td>
      </tr>
      <tr>
        <th colspan="2">Rates</th>
        <td title="True Positive Rate (OR hit rate OR recall OR sensitivity) = TP/(TP+FN)">TPR=#echo "%.2f"%$t.TPR#</td>  
        <td title="False Positive Rate = FP/(TN+FP)">FPR=#echo "%.5f"%$t.FPR#</td>
        <td title="This square is intentionally left blank"></td>
        <td title="Accuracy = (TP+TN)/(TP+TN+FP+FN)">Acc=#echo "%.5f"%t.accuracy#</td>
      </tr>
    </tbody>
  </table>

  <h2>Common Performance Measures
  <button class="toggle" id="toggle_h_performance">?</button>
  </h2>

  <div class="help" id="h_performance">
    <p>These are performance measures which are commonly quoted.  All of
    them are derived from the confusion matrix.  In this context, Recall,
    Precision and False Positive Rate are the most relevant.
  
    <dl>
      <dt>Prevalence</dt>
      <dd>
        Proportion of training data which was positive
      </dd>
      
      <dt>Accuracy</dt>
      <dd>
        Proportion of correct predictions
      </dd>

      <dt>Error Rate (1-Accuracy)</dt>
      <dd>
        Proportion of incorrect predictions
      </dd>

      <dt>Recall OR True Positive Rate OR Sensitivity</dt>
      <dd>
        Proportion of positives which were correctly predicted to be positive.
      </dd>

      <dt>Precision OR Positive Predictive Value</dt>
      <dd>
        Proportion of predicted positives which are true positives.
      </dd>

      <dt>Negative Predictive Value</dt>
      <dd>
        Proportion of predicted negatives which are true negatives.
      </dd>

      <dt>Specificity OR True Negative Rate (1-False Positive Rate)</dt>
      <dd>
        Proportion of negatives which were correctly predicted to be negative.
      </dd>

      <dt>False Positive Rate (1-Specificity)</dt>
      <dd>
        Proportion of negatives which were incorrectly predicted to be positive.
      </dd>
      
      <dt>False Discovery Rate (1-Precision)</dt>
      <dd>
        Proportion of predicted positives which are false positives.
      </dd>

      <dt>FP/TP Ratio</dt>
      <dd>
        Ratio of false positives to true positives.  Related to precision: 
        if TP/FP is <i>x</i>, then precision is <i>1/(x+1)</i>.
      </dd>

      <dt>&alpha;-Weighted F-Measure</dt>
      <dd>
        The threshold is chosen to maximise this quantity. 0 &lt;=
        &alpha; &lt;= 1 controls the weight of precision.  When
        &alpha;=0.5 precision and recall are weighted equally
        (identical to F1-Measure).
      </dd>

      <dt>F1-Measure</dt>
      <dd>
        Harmonic mean of recall and precision at the threshold
        corresponding to the maximum &alpha;-weighted F-Measure.
      </dd>

      <dt>Maximum F1-Measure</dt>
      <dd>
        Maximum possible value of F1-Measure (as achieved when using
        &alpha;=0.5).
      </dd>

      <dt>Enrichment</dt>
      <dd>
        Precision over prevalence.  This is is how much better this
        classifier's precision is over a classifier which calls
        everything positive.
      </dd>
      
  </dl>
  </div>

  <table class="results">
    <colgroup>
      <col class="longlabel">
      <col class="value">
    </colgroup>
    <tr>
      <th><b>Prevalence</b> 
      <span class="footnote">P/(P+N)</span></th>
      <td>#echo "%.5f" % $t.prevalence #</td>
    </tr>
    <tr>
      <th><b>Accuracy</b> 
      <span class="footnote">(TP+TN)/(P+N)</span></th>
      <td>#echo "%.5f" % $t.accuracy #</td>
    </tr>
    <tr>
      <th><b>Error Rate</b> 
      <span class="footnote">(FP+FN)/(P+N)=1-Accuracy</span></th>
      <td>#echo "%.5f" % (1-$t.accuracy) #</td>
    </tr>
    <tr>
      <th><b>Recall OR Sensitivity OR True Positive Rate</b> 
      <span class="footnote">&rho;=TPR=TP/(TP+FN)</span></th>
      <td>#echo "%.5f" % $t.recall #</td>
    </tr>
    <tr>
      <th><b>Precision OR Positive Predictive Value</b> 
      <span class="footnote">&pi;=PPV=TP/(TP+FP)</span></th>
      <td>#echo "%.5f" % $t.PPV #</td>
    </tr>
    <tr>
      <th><b>False Discovery Rate</b>
      <span class="footnote">FDR=FP/(TP+FP)=1-Precision</span></th>
      <td>#echo "%.5f" % $t.FDR #</td>
    </tr>
    <tr>
      <th><b>Specificity OR True Negative Rate</b> 
      <span class="footnote">TNR=TN/(TN+FP)=1-FPR</span></th>
      <td>#echo "%.5f" % $t.TNR #</td>
    </tr>
    <tr>
      <th><b>False Positive Rate</b> 
      <span class="footnote">FPR=FP/(TN+FP)=1-TNR</span></th>
      <td>#echo "%.5f" % $t.FPR #</td>
    </tr>
    <tr>
      <th><b>Negative Preditive Value</b> 
      <span class="footnote">NPV=TN/(TN+FN)</span></th>
      <td>#echo "%.5f" % $t.NPV #</td>
    </tr>
    <tr>
      <th><b>FP/TP Ratio</b></th>
      <td>#echo "%.5f" % t.fp_tp_ratio #</td>
    </tr>
    <tr>
      <th><b>Weighted F-Measure</b> 
      <span class="footnote">(1/(&alpha;/&pi;+(1-&alpha;)/&rho;))</span></th>
      <td>#echo "%.5f" % $t.fmeasure_alpha #</td>
    </tr>
    <tr>
      <th><b>F1-Measure</b> 
      <span class="footnote">(2*&rho;*&pi;/(&rho;+&pi;))</span></th>
      <td>#echo "%.5f" % $t.fmeasure #</td>
    </tr>
    <tr>
      <th><b>Maximum F1-Measure</b></th>
      <td>#echo "%.5f" % $t.fmeasure_max #</td>
    </tr>
    <tr>
      <th><b>Enrichment</b> 
      <span class="footnote">(= precision/prevalence)</span></th>
      <td>#echo "%.5f" % $t.enrichment #</td>
    </tr>
  </table>

  <h2>Performance graphs
  <button class="toggle" id="toggle_h_graphs">?</button>
  </h2>

  <div class="help" id="h_graphs">
    <dl>
      <dt>Article Score Densities</dt>
      <dd>
        Histograms of article scores for negative and positive
        articles.  Classifier performs better when the distributions
        are well-separated. Tuned threshold is marked with a vertical
        line.
      </dd>
      <dt>Feature Score Density</dt>
      <dd>
        Histogram of feature scores.
      </dd>
      <dt>ROC Curve</dt>
      <dd>
        True Positive Rate (recall) against False Positive Rate
        (1-specificity).  The greater the area under the ROC curve,
        the better the global performance of the classifier.  In the
        worst case, true positives increase at the same rate as false
        positives, yielding a 45-degree line.
      </dd>
      <dt>Precision-Recall Curve</dt>
      <dd>
        Precision against Recall.  In good cases, the precision only
        drops off at high recall.  Tuned threshold is marked with a
        vertical line.  Worst case is a horizontal line with precision
        equal to prevalence.
      </dd>
      <dt>Variation against Threshold</dt>
      <dd>
        Precision, recall and F-measure against threshold shows how
        the threshold was optimised for &alpha;-weighted F-Measure and
        how changing the threshold would affect the recall/precision
        balance.
      </dd>
    </dl>
  </div>

  <img src="$rc.report_artscores_img" alt="Article Score Densities"/>
  
  <img src="$rc.report_featscores_img" alt="Feature Score Density"/>
  
  <img src="$rc.report_roc_img" alt="ROC Curve"/>
  
  <img src="$rc.report_prcurve_img" alt="Precision-Recall Curve"/>
  
  <img src="$rc.report_fmeasure_img" alt="Precision and Recall vs Threshold"/>

  #include str($rc.templates/"invalid.tmpl")

</div><!--narrow-->

</div><!--body-->

<div id="footer">
                                     
</div>

</body>
</html>