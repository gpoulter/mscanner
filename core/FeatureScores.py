"""Calculates feature scores from occurrence counts"""

from __future__ import division
import numpy as nx

from mscanner import update, delattrs
from mscanner.core.Storage import Storage


                                     
__author__ = "Graham Poulter"                                        
__license__ = """This program is free software: you can redistribute it and/or
modify it under the terms of the GNU General Public License as published by the
Free Software Foundation, either version 3 of the License, or (at your option)
any later version.

This program is distributed in the hope that it will be useful, but WITHOUT ANY
WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A
PARTICULAR PURPOSE. See the GNU General Public License for more details.

You should have received a copy of the GNU General Public License along with
this program. If not, see <http://www.gnu.org/licenses/>."""


class FeatureScores(object):
    """Feature score calculation and saving, with choice of calculation method,
    and methods to exclude certain kinds of features.
    
    @group Set via constructor: featmap, pseudocount, scorefunction, mask
    
    @ivar featmap: L{FeatureMapping} object
    
    @ivar pseudocount: Prior count to use for features, or None to use prior
    counts equal to Medline frequency.
    
    @ivar scoremethod: Name of the method for calculating feature scores.
    
    @ivar maskfunc: Function taking L{FeatureScores} as a parameter, and 
    returning a boolean array for setting L{mask}.
       
    
    @group Set by update: pos_counts, neg_counts, pdocs, ndocs, prior
    
    @ivar pos_counts: Array of feature counts in positive documents.
    
    @ivar neg_counts: Array of feature counts in negatives documents.
    
    @ivar pdocs: Number of positive documents.
    
    @ivar ndocs: Number of negative documents.
    
    @ivar prior: Bayes prior to add to the score.  If None, estimate
    using the ratio of relevant to irrelevant articles in the data.
    
    @ivar mask: Boolean array for feature selection.  True positions indicate 
    features to use, false positions are features that we pretend do not exist.

    
    @group Set via scoremethod: scores, pfreqs, nfreqs, base
    
    @ivar scores: Score of each feature.
  
    @ivar pfreqs: Numerator of score fraction.
    
    @ivar nfreqs: Denominator of score fraction.
    
    @ivar base: Value to be added to all article scores (typically the
    score of an article with zero features).
    """

    def __init__(self, 
                 featmap,
                 pseudocount=None,
                 scoremethod="scores_bayes",
                 maskfunc=lambda x:None):
        """Initialise FeatureScores object (parameters are instance variables)"""
        if isinstance(scoremethod, basestring):
            scoremethod = getattr(self, scoremethod)
        prior = 0
        update(self, locals())


    @staticmethod
    def Defaults(featmap):
        """Create a FeatureScores parameterised using the RC defaults.
        @param featmap: L{FeatureMapping} instance to use."""
        from mscanner.configuration import rc
        def maskfunc(fs):
            # Exclude certain classes of features (word/mesh/issn)
            r = fs.featmap.class_mask(rc.class_mask)
            # Exclude features not occurring enough times in the data
            r |= ((fs.pos_counts + fs.neg_counts) < rc.mincount)
            #r |= fs.pos_counts < rc.mincount
            return r
        return FeatureScores(
            featmap = featmap,
            pseudocount = rc.pseudocount,
            scoremethod = rc.scoremethod,
            maskfunc = maskfunc)


    def scores_of(self, featdb, pmids):
        """Calculate vector of scores given an iterable of PubMed IDs.
        
        @param featdb: Mapping from PMID to feature vector
        @param pmids: Iterable of keys into L{featdb}
        @return: Vector containing document scores corresponding to the pmids.
        """
        off = self.base + self.prior
        sc = self.scores
        return nx.array([off+nx.sum(sc[featdb[d]]) for d in pmids], nx.float32)
    

    def __len__(self):
        """Number of features"""
        return len(self.featmap)


    def update(self, pos_counts, neg_counts, pdocs, ndocs, prior=None):
        """Change the feature counts and numbers of documents, clear
        old score calculations, and calculate new scores."""
        if prior is None:
            if pdocs == 0 or ndocs == 0:
                prior = 0
            else:
                prior = nx.log(pdocs/ndocs)
        base = 0
        update(self, locals())
        self.scoremethod()
        self._apply_mask()
        delattrs(self, "_stats", "_tfidf")


    def scores_bayes(s):
        """Estimate support scores of features assuming documents are 
        generated by a multivariate Bernoulli distribution.  Feature 
        non-occurrence is modeled as a base score for the document with
        no features, and an adjustment to the feature occurrence scores."""
        s._make_pseudovec()
        # Posterior term frequencies in relevant articles
        s.pfreqs = (s.pseudocount+s.pos_counts) / (1+s.pdocs)
        # Posterior term frequencies in irrelevant articles
        s.nfreqs = (s.pseudocount+s.neg_counts) / (1+s.ndocs)
        # Support scores for bernoulli successes
        s.present_scores = nx.log(s.pfreqs/s.nfreqs)
        # Support scores for bernoulli failures
        s.absent_scores = nx.log( (1-s.pfreqs) / (1-s.nfreqs) )
        # Conversion to base score (no terms) and occurrence score
        s.base = nx.sum(s.absent_scores)
        s.scores = s.present_scores - s.absent_scores


    def scores_noabsence(s):
        """Like scores_bayes, but ignoring non-occurring features, resulting
        in higher document scores (the ranking is almost the same though)."""
        s.base = 0
        s._make_pseudovec()
        s.pfreqs = (s.pseudocount+s.pos_counts) / (1+s.pdocs)
        s.nfreqs = (s.pseudocount+s.neg_counts) / (1+s.ndocs)
        s.scores = nx.log(s.pfreqs) - nx.log(s.nfreqs)


    def scores_rubin(s):
        """Estimates a "log likelihood ratio" as a product of the likelihood
        ratios for each occurring features, where zero probabilities are 
        replaced with 1e-8."""
        s.base = 0
        s.pseudocount = 0
        s.pfreqs = s.pos_counts / float(s.pdocs)
        s.nfreqs = s.neg_counts / float(s.ndocs)
        s.pfreqs[s.pfreqs == 0.0] = 1e-8
        s.nfreqs[s.nfreqs == 0.0] = 1e-8
        s.scores = nx.log(s.pfreqs) - nx.log(s.nfreqs)


    def _make_pseudovec(s):
        """Calculates a pseudocount vector of background frequencies,
        if no constant pseudocount was specified."""
        if s.pseudocount is None:
            s.pseudocount = \
             nx.array(s.featmap.counts, nx.float32) / s.featmap.numdocs


    def _apply_mask(self):
        """Uses the result of L{maskfunc} to exclude certain features"""
        mask = self.maskfunc(self)
        self.mask = mask
        if mask is not None:
            self.pos_counts[mask] = 0
            self.neg_counts[mask] = 0
            self.pfreqs[mask] = 0
            self.nfreqs[mask] = 0
            self.scores[mask] = 0


    @property 
    def stats(self):
        """A Storage instance with statistics about the features
        
        The following keys are present:
            - pos_occurrences: Total feature occurrences in positives
            - neg_occurrences: Total feature occurrences in negatives
            - feats_per_pos: Number of features per positive article
            - feats_per_neg: Number of features per negative article
            - distinct_feats: Number of distinct features
            - pos_distinct_feats: Number of of distinct features in positives
            - neg_distinct_feats: Number of of distinct features in negatives
        """
        try: 
            return self._stats
        except AttributeError: 
            pass
        s = Storage()
        s.pdocs = self.pdocs
        s.ndocs = self.ndocs
        s.num_feats = len(self)
        s.pos_occurrences = int(nx.sum(self.pos_counts)) 
        s.feats_per_pos = 0.0
        if self.pdocs > 0:
            s.feats_per_pos = s.pos_occurrences / s.pdocs 
        s.neg_occurrences = int(nx.sum(self.neg_counts))
        s.feats_per_neg = 0.0
        if self.ndocs > 0:
            s.feats_per_neg = s.neg_occurrences / s.ndocs 
        s.pos_distinct_feats = len(nx.nonzero(self.pos_counts)[0]) 
        s.neg_distinct_feats = len(nx.nonzero(self.neg_counts)[0]) 
        self._stats = s
        return self._stats


    @property
    def tfidf(self):
        """Vector of TF-IDF scores for each feature
        
        Cache TF-IDF scores for terms, where for term frequency (TF) we treat
        the positive corpus as a single large document, but for inverse
        document frequency (IDF) each citation is a separate document."""
        try: 
            return self._tfidf
        except AttributeError: 
            pass
        self._tfidf = nx.zeros(len(self.pos_counts), dtype=float)
        # Document frequency
        docfreq_t = self.pos_counts+self.neg_counts
        # Number of documents
        N = self.pdocs+self.ndocs # number of documents
        # Mask to exclude zero-frequency features
        u = (docfreq_t != 0) 
        # Inverse Document Frequency (log N/df_t)
        idf = nx.log(N / docfreq_t[u])
        # Term frequency
        tf = (self.pos_counts[u] / nx.sum(self.pos_counts))
        # Calculate TF.IDF
        self._tfidf[u] = tf * idf
        return self._tfidf


    def get_best_tfidfs(self, count):
        """Construct a table about the features with the best TF.IDF.
        
        @param count: Maximum of rows to return.
        
        @return: List of (Feature ID, TFIDF, (feature, featureclass), 
        feature score, positive count, negative count)."""
        from heapq import nlargest
        best_tfidfs = nlargest(
            count, enumerate(self.tfidf), key=lambda x:x[1])
        return [ (t, tfidf, self.featmap[t], self.scores[t], 
                  self.pos_counts[t], self.neg_counts[t])
                  for t, tfidf in best_tfidfs ]


    def write_csv(self, stream):
        """Write features scores as CSV to an output stream"""
        stream.write(u"score,positives,negatives,numerator,"\
                     u"denominator,pseudocount,termid,tfidf,type,term\n")
        s = self
        s.tfidf
        if not isinstance(s.pseudocount, nx.ndarray):
            pseudocount = nx.zeros_like(s.scores) + float(s.pseudocount)
        else:
            pseudocount = s.pseudocount
        for t, score in sorted(
            enumerate(s.scores), key=lambda x:x[1], reverse=True):
            if (s.mask is not None) and s.mask[t]:
                continue
            stream.write(
                u'%.3f,%d,%d,%.2e,%.2e,%.2e,%d,%.2f,%s,"%s"\n' % 
                (s.scores[t], s.pos_counts[t], s.neg_counts[t], 
                 s.pfreqs[t], s.nfreqs[t], pseudocount[t], t,
                 s.tfidf[t], s.featmap[t][1], s.featmap[t][0]))



def FeatureCounts(nfeats, featdb, docids):
    """Count occurrenes of each feature in a set of articles

    @param nfeats: Number of distinct features (length of L{docids})

    @param featdb: Mapping from document ID to array of feature IDs

    @param docids: Iterable of document IDs whose features are to be counted

    @return: Array of length L{nfeats}, containing occurrence count of each feature
    """
    counts = nx.zeros(nfeats, nx.int32)
    for docid in docids:
        counts[featdb[docid]] += 1
    return counts