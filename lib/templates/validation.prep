<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">

<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">

<!--
p - Global performance stats object
t - Tuned performance stats object
f - Feature information object
c - Configuration module
-->

<head>

  <title>MScanner validation results for {{c.dataset}}</title>

  <link rel="stylesheet" type="text/css" href="style.css"/>

  <style type="text/css">
    td.tp { background-color: #FF3333; }
    td.fn { background-color: #FFCCCC; }
    td.tn { background-color: #3333FF; }
    td.fp { background-color: #CCCCFF; }
  </style>

  <script type="text/javascript">
    current = null;
    function hide(id) {
    if (id == null) return;
    current = null;
    document.getElementById(id).style.display = "none";
    }
    function show(id) {
    hide(current);
    current = id;
    document.getElementById(id).style.display = "block";
    }
    function toggle(id) {
    dstyle = document.getElementById(id).style;
    if (dstyle.display == "none") {
    dstyle.display = "block";
    }
    else {
    dstyle.display = "none";
    }
    }
  </script>

</head>

<body>

<h1 id="heading">MScanner validation results for {{c.dataset}}</h1>

<div id="content">

  <div class="box">
  
    <h2 class="heading">Related Files</h2>

    <table>
      <tr>
        <th>
          Timestamp
        </th>
        <td>
          {{time}}
        </td>
      </tr>
      <tr>
        <th>
          Positive data (.txt)
          <button onclick="show('positives_help')">?</button>
        </th>
        <td><a href="{{c.posfile}}">{{c.posfile}}</a></td>
      </tr>
      <tr>
        <th>
          Negative data (.txt)
          <button onclick="show('negatives_help')">?</button>
        </th>
        <td><a href="{{c.negfile}}">{{c.negfile}}</a></td>
      </tr>
      <tr>
        <th>
          Term scores (.csv)
          <button onclick="show('csv_help')">?</button>
        </th>
        <td><a href="{{c.term_scores_name}}">{{c.term_scores_name}}</a></td>
      </tr>
      <tr>
        <th>
          Pseudocount
          <button onclick="show('pseudocount_help')">?</button>
        </th>
        <td>
        {{if isinstance(c.pseudocount, float)}}
        {{"%f"%c.pseudocount}}
        {{else}}
        per-feature
        {{endif}}
        </td>
      </tr>
      <tr>
        <th>
          Number of folds
          <button onclick="show('nfold_help')">?</button>
        </th>
        <td>{{"%d"%c.nfolds}}</td>
      </tr>
      <tr>
        <th>
          F-Measure Alpha
          <button onclick="show('alpha_help')">?</button>
        </th>
        <td>{{"%f"%c.alpha}}</td>
      </tr>
      <tr>
        <th>
          Tuned threshold
          <button onclick="show('threshold_help')">?</button>
        </th>
        <td>{{"%.5f"%t.threshold}}</td>
      </tr>
      <tr>
        <th>
          Area under ROC curve
          <button onclick="show('roc_area_help')">?</button>
        </th>
        <td>{{"%.5f"%p.ROC_area}}</td>
      </tr>
      <tr>
        <th>
          Area under precision-recall curve
          <button onclick="show('pr_area_help')">?</button>
        </th>
        <td>{{"%.5f"%p.PR_area}}</td>
      </tr>
      {{if overlap is not None}}
      <tr>
        <th>
          Overlap in score densities
          <button onclick="show('overlap_help')">?</button>
        </th>
        <td>{{"%.5f"%overlap}}</td>
      </tr>
      {{endif}}
    </table>

    <div class="help" id="positives_help">
      This file contains a list of the PubMed IDs of the positive
      training documents.
    </div>

    <div class="help" id="negatives_help">
      This file contains a list of the PubMed IDs of the negative
      training documents, which were randomly selected.  The number of
      PubMed IDs in this file may be greater than the number of
      negative documents below, because after reading the file any
      PubMed IDs overlapping with the positive training documents are
      filtered out (hence this the actual negatives used are
      negatives.txt minus anything in positives.txt).
    </div>

    <div class="help" id="csv_help">
      A CSV table (which Excel can import) with the calculation of
      each MeSH term's score.  The score of a MeSH term is the natural
      logarithm of the ratio of the term frequency amongst the
      positive documents to frequency of the term in the rest of
      Medline.  Each article has a number of MeSH terms associated
      with it, and the score for an article is the sum of the scores
      of its MeSH terms.
      below.
    </div>
    
    <div class="help" id="pseudocount_help">
      Added to the number of occurrences of every term, to avoid divide
      by zero on terms which don't occur anywhere in a set of articles.
    </div>
    
    <div class="help" id="nfold_help">
      <p>
        The positive and negative sets are each paritions into K
        random test sets. K-1 of test sets are used to train the
        classifier, and scores calculated for the remaining test set,
        and this is repeated K times.  Thus, when the classifier
        assigns a score to a document, it has has not already "seen"
        the document in training.
      </p>
      <p>
        When K is zero, leave-out-one validation is used instead, in
        which all but one of the documents are used to train the
        classifier which then scores the last document.  This is
        important, as training on that last document may give it an
        artificially higher score.
      </p>
    </div>
    
    <div class="help" id="alpha_help">
      Alpha is used in the &alpha;-weighted F-Measure which is
      maximised to choose the decision threshold
      FM=1/(alpha/P+(1-alpha)/R) which balances recall (R) and
      precision (P).  &alpha;=0.5 corresponds to the standard
      F-Measure (harmonic mean of precision and recall), while for
      0.5&lt;alpha&lt;1.0 precision is weighted more highly, so that
      during optimisation some recall will be traded for increased
      precision.
    </div>
    
    <div class="help" id="threshold_help">
      After cross-validation assigned scores to all of the articles in
      the positive and negative sets, the threshold score is tuned to
      maximise precision subject to the constraint that the F-measure
      should be at least 90% of the maximum F-measure.  The line for
      this tuned threshold is marked on all of the graphs.
    </div>

    <div class="help" id="roc_area_help">
      The area under the ROC curve is a global measure of classifier
      performance.  The closer to 1.0, the better the classifier.  A
      classifier which randomly assigned scores would have a linear
      ROC curve, with an area of 0.5.
    </div>

    <div class="help" id="pr_area_help">
      The area under the Precision-Recall (PR) curve is a global
      measure of classifier performance.  The closer to 1.0, the
      better the classifier.  Although it is independent of threshold, it 
      still correlates with prevalence so it not directly comparable between 
      inputs of different size.
    </div>

    <div class="help" id="overlap_help">
      The overlap between score distributions, is the area of the score 
      density distributions (the top graph below) which overlap.  Since the area is 
      populated by a mixture of positive and negative items, the classifier has to
      make a tradeoff between recall and precision when choosing the decision threshold.  
      The larger the area of overlap, the worse one can expect classifier 
      performance to be.
    </div>
  </div>

  <div class="box">
    <h2 class="heading">Feature Statistics</h2>
    <table id="summary">
      <thead>
        <tr>
          <th>Quantity</th><th>Positives</th><th>Negatives </th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>Number of documents</th>
          <td>{{f.pdocs}}</td>
          <td>{{f.ndocs}}</td>
        </tr>
        <tr>
          <th>Number of distinct features</th>
          <td>{{f.pos_distinct_feats}}</td>
          <td>{{f.neg_distinct_feats}}</td>
        </tr>
        <tr>
          <th>Total feature occurrences</th>
          <td>{{f.pos_occurrences}}</td>
          <td>{{f.neg_occurrences}}</td>
        </tr>
        <tr>
          <th>Terms per document</th>
          <td>{{"%.3f" % f.feats_per_pos}}</td>
          <td>{{"%.3f" % f.feats_per_neg}}</td>
        </tr>
      </tbody>
    </table>
  </div>

  <div class="box">

    <h2 class="heading">
      Raw classifier counts
      <button onclick="toggle('counts_help')">?</button>
    </h2>

    <table>
      <thead>
        <tr>
          <th></th>
          <th>Correct</th>
          <th>Incorrect</th>
          <th>Total</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>Positive</th>
          <td class="tp">TP={{"%d"%t.TP}}</td>
          <td class="fn">FN={{"%d"%t.FN}}</td>
          <td>P={{"%d"%t.P}}</td>
        </tr>
        <tr>
          <th>Negative</th>
          <td class="tn">TN={{"%d"%t.TN}}</td>
          <td class="fp">FP={{"%d"%t.FP}}</td>
          <td>N={{"%d"%t.N}}</td>
        </tr>
        <tr>
          <th>Total</th>
          <td>T={{"%d"%t.T}}</td>
          <td>F={{"%d"%t.F}}</td>
          <td>A={{"%d"%t.A}}</td>
        </tr>
      </tbody>
    </table>

    <div class="help" id="counts_help">
      <ul>
        <li>TP = True Positives</li>
        <li>FP = False Positives</li>
        <li>TN = True Negatives</li>
        <li>FN = False Negatives</li>
        <li>P = Number of positives</li>
        <li>N = Number of negatives</li>
        <li>A = Total number of articles</li>
        <li>T = Correctly classified</li>
        <li>F = Incorrectly classified</li>
      </ul>
    </div>
    
  </div>

  <div class="box">

    <h2 class="heading">
      Proportional measures
      <button onclick="toggle('proportions_help')">?</button>
    </h2>

    <table border="1">
      <thead>
        <tr>
          <th></th>
          <th>Correct</th>
          <th>Incorrect</th>
          <th>Total</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>Positive</th>
          <td class="tp">TPR={{"%.2f"%t.TPR}}</td>
          <td class="fn">FNR={{"%.2f"%t.FNR}}</td>
          <td>{{"%.2f"%(t.TPR+t.FNR)}}</td>
        </tr>
        <tr>
          <th>Negative</th>
          <td class="tn">TNR={{"%.2f"%t.TNR}}</td>
          <td class="fp">FPR={{"%.2f"%t.FPR}}</td>
          <td>{{"%.2f"%(t.TNR+t.FPR)}}</td>
        </tr>
        <tr>
          <th>Total</th>
          <td>{{"%.2f"%(t.TPR+t.TNR)}}</td>
          <td>{{"%.2f"%(t.FNR+t.FPR)}}</td>
          <td>{{"%.2f"%(t.TPR+t.FNR+t.TNR+t.FPR)}}</td>
        </tr>
      </tbody>
    </table>

    <div class="help" id="proportions_help">
      <ul>
        <li>TPR = True Positive Rate (TP/P)</li>
        <li>FNR = False Negative Rate (FN/P)</li>
        <li>FPR = False Positive Rate (FP/N)</li>
        <li>TNR = True Negative Rate (TN/N)</li>
      </ul>
    </div>
    
  </div>

  <div class="box">

    <h2 class="heading">
      Performance Measures
      <button onclick="toggle('derived_help')">?</button>
    </h2>

    <table>
      <tr>
        <th><b>Prevalence</b> <span class="footnote">(=P/A)</span></th>
        <td>{{"%.5f"%t.prevalence}}</td>
      </tr>
      <tr>
        <th><b>Accuracy</b> <span class="footnote">(=T/A)</span></th>
        <td>{{"%.5f"%t.accuracy}}</td>
      </tr>
      <tr>
        <th><b>Error Rate</b> <span class="footnote">(=F/A=1-Accuracy)</span></th>
        <td>{{"%.5f"%(1-t.accuracy)}}</td>
      </tr>
      <tr>
        <th><b>Positive Predictive Value</b> <span class="footnote">(= PPV = TP/(TP+FP))</span></th>
        <td>{{"%.5f"%t.PPV}}</td>
      </tr>
      <tr>
        <th><b>Negative Preditive Value</b> <span class="footnote">(= NPV = TN/(TN+FN))</span></th>
        <td>{{"%.5f"%t.NPV}}</td>
      </tr>
      <tr>
        <th><b>Recall</b> <span class="footnote">(= &rho; = TPR = sensitivity = TP/P)</span></th>
        <td>{{"%.5f"%t.recall}}</td>
      </tr>
      <tr>
        <th><b>Precision</b> <span class="footnote">(= &pi; = PPV = TP/(TP+FP))</span></th>
        <td>{{"%.5f"%t.precision}}</td>
      </tr>
      <tr>
        <th><b>Break-Even</b> <span class="footnote">(where precision=recall)</span></th>
        <td>{{"%.5f"%p.breakeven}}</td>
      </tr>
      <tr>
        <th><b>Weighted F-Measure</b> <span class="footnote">(1/(&alpha;/&pi;+(1-&alpha;)/&rho;))</span></th>
        <td>{{"%.5f"%t.fmeasure_alpha}}</td>
      </tr>
      <tr>
        <th><b>F1-Measure</b> <span class="footnote">(2*&rho;*&pi;/(&rho;+&pi;))</span></th>
        <td>{{"%.5f"%t.fmeasure}}</td>
      </tr>
      <tr>
        <th><b>Maximum F1-Measure</b></th>
        <td>{{"%.5f"%t.fmeasure_max}}</td>
      </tr>
      <tr>
        <th><b>Enrichment</b> <span class="footnote">(= precision/prevalence)</span></th>
        <td>{{"%.5f"%t.enrichment}}</td>
      </tr>
    </table>

    <div class="help" id="ir_help">
      <ul>
        <li>
          Prevalence = Proportion of data which was positive
        </li>
        <li>
          Accuracy = Proportion of correct classifications
        </li>
        <li>
          Error Rate = Proportion of incorrect classifications
          (1-Accuracy).
        </li>
        <li>
          PPV = Proportion of those classified positive which actually were positive
        </li>
        <li>
          NPV = Proportion of those classified negative which actually were negative
        </li>
        <li>
          Recall = Proportion of positives which were classified
          correctly (same thing as true positive rate). That is, it is
          the proportion of positives which the classifier managed to
          recover during testing.
        </li>
        <li>
          Precision = Proportion of those classified positive which
          actually were positive.
        </li>
        <li>
          BreakEven = The break-even point, which is the value at
          which precision equals recall (equals F1-Measure).  This is
          value is usually somewhat lower than the maximum F1-Measure.
        </li>
        <li>
          Weighted F-Measure = Parameter &alpha; weights precision or
          recall more highly. The weighted F-Measure is used for
          tuning the threshold.  When &alpha;=0.5, the weighted
          F-Measure equals the F1-Measure.
        </li> 
        <li>
          F1-Measure = Harmonic mean of recall and precision
        </li>
        <li>
          Maximum F1-Measure = Maximum possible F1-Measure (weighted
          F-Measure achieves this only for &alpha;=0.5).
        </li>
        <li>
          Enrichment = Precision over prevalence.  This is how many
          times the precision is enriched compared to a classifier
          which called everything positive.  The
        </li>
      </ul>
    </div>

  </div>

  <div class="box">

    <h2 class="heading">
      Performance graphs
      <button onclick="toggle('graph_help')">?</button>
    </h2>

    <div class="help" id="graph_help">
      <ul>
        <li>
          The first graph is a score histogram comparing the
          distributions of the background ("negative") articles to
          those of the query ("positive") articles.  When the
          distributions are well-separated, the classifier performance
          is better than when the distributions largely overlap.
        </li>
        <li>
          The second graph is an ROC curve plotting the True Positive
          Rate (or recall) against the False Positive Rate (which is
          1-specificity).  The greater the area under the curve, the
          better the global performance of the classifier.
        </li>
        <li>
          The precision recall curve plots precision against recall.
          Good performance is when the precision remains high at high
          recall.
        </li>
        <li>
          The precision, recall and F-measure against threshold is like the
          previous graph, but shows how tuning the threshold would affect
          the results.  
        </li>
        <li>
          All of the graphs have a vertical line marking the tuned
          threshold.
        </li>
      </ul>
    </div>

    <img src="{{c.hist_img}}" alt="Article Score Densities"/>
    
    <img src="{{c.feat_img}}" alt="Feature Score Density"/>
    
    <img src="{{c.roc_img}}" alt="ROC Curve"/>
    
    <img src="{{c.p_vs_r_img}}" alt="Precision-Recall Curve"/>
    
    <img src="{{c.pr_vs_score_img}}" alt="Precision and Recall vs Threshold"/>

  </div>
  
</div>

</body>
</html>