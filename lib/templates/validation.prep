<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">

<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">

<head>

  <title>MedScanner Performance Statistics</title>

  <link rel="stylesheet" type="text/css" href="style.css"/>

  <style type="text/css">
    td.tp { background-color: #FF3333; }
    td.fn { background-color: #FFCCCC; }
    td.tn { background-color: #3333FF; }
    td.fp { background-color: #CCCCFF; }
  </style>

  <script type="text/javascript">
    current = null;
    function hide(id) {
    if (id == null) return;
    current = null;
    document.getElementById(id).style.display = "none";
    }
    function show(id) {
    hide(current);
    current = id;
    document.getElementById(id).style.display = "block";
    }
    function toggle(id) {
    dstyle = document.getElementById(id).style;
    if (dstyle.display == "none") {
    dstyle.display = "block";
    }
    else {
    dstyle.display = "none";
    }
    }
  </script>

</head>

<body>

<h1 id="heading">Medline Scanner Performance Statistics</h1>

<div id="content">

  <div class="box">
  
    <h2 class="heading">Related Files</h2>

    <table>
      <tr>
        <th>
          Term scores (HTML)
          <button onclick="show('terms_help')">?</button>
        </th>
        <td><a href="{{terms_html}}">{{terms_html}}</a></td>
      </tr>
      <tr>
        <th>
          Term scores (CSV)
          <button onclick="show('csv_help')">?</button>
        </th>
        <td><a href="{{terms_csv}}">{{terms_csv}}</a></td>
      </tr>
      <tr>
        <th>
          Tuned threshold
          <button onclick="show('threshold_help')">?</button>
        </th>
        <td>{{"%.5f"%threshold}}</td>
      </tr>
      <tr>
        <th>
          Area under ROC curve
          <button onclick="show('roc_area_help')">?</button>
        </th>
        <td>{{"%.5f"%ROC_area}}</td>
    </table>

    <div class="help" id="terms_help">
      HTML display of MeSH term scores. This file can be very large
      (2Mb).
    </div>
    
    <div class="help" id="csv_help">
      A CSV table (which Excel can import) with the calculation of
      each MeSH term's score.  The score of a MeSH term is the natural
      logarithm of the ratio of the term frequency amongst the
      positive documents to frequency of the term in the rest of
      Medline.  Each article has a number of MeSH terms associated
      with it, and the score for an article is the sum of the scores
      of its MeSH terms.  This file is more compact than the HTML
      below.
    </div>
    
    <div class="help" id="threshold_help">
      After cross-validation assigned scores to all of the articles in
      the positive and negative sets, the threshold score is tuned to
      maximise precision subject to the constraint that the F-measure
      should be at least 90% of the maximum F-measure.  The line for
      this tuned threshold is marked on all of the graphs.
    </div>

    <div class="help" id="roc_area_help">
      The area under the ROC curve is a global measure of classifier
      performance.  The closer to 1.0, the better the classifier.  A
      classifier which randomly assigned scores would have a linear
      ROC curve, with an area of 0.5.
    </div>

  </div>

  <div class="box">

    <h2 class="heading">
      Raw classifier counts
      <button onclick="toggle('counts_help')">?</button>
    </h2>

    <table>
      <thead>
        <tr>
          <th></th>
          <th>Correct</th>
          <th>Incorrect</th>
          <th>Total</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>Positive</th>
          <td class="tp">TP={{TP}}</td>
          <td class="fn">FN={{FN}}</td>
          <td>P={{P}}</td>
        </tr>
        <tr>
          <th>Negative</th>
          <td class="tn">TN={{TN}}</td>
          <td class="fp">FP={{FP}}</td>
          <td>N={{N}}</td>
        </tr>
        <tr>
          <th>Total</th>
          <td>T={{T}}</td>
          <td>F={{F}}</td>
          <td>A={{A}}</td>
        </tr>
      </tbody>
    </table>

    <div class="help" id="counts_help">
      <ul>
        <li>TP = True Positives</li>
        <li>FP = False Positives</li>
        <li>TN = True Negatives</li>
        <li>FN = False Negatives</li>
        <li>P = Number of positives</li>
        <li>N = Number of negatives</li>
        <li>A = Total number of articles</li>
        <li>T = Correctly classified</li>
        <li>F = Incorrectly classified</li>
      </ul>
    </div>
    
  </div>

  <div class="box">

    <h2 class="heading">
      Proportional measures
      <button onclick="toggle('proportions_help')">?</button>
    </h2>

    <table border="1">
      <thead>
        <tr>
          <th></th>
          <th>Correct</th>
          <th>Incorrect</th>
          <th>Total</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>Positive</th>
          <td class="tp">TPR={{"%.2f"%TPR}}</td>
          <td class="fn">FNR={{"%.2f"%FNR}}</td>
          <td>{{"%.2f"%(TPR+FNR)}}</td>
        </tr>
        <tr>
          <th>Negative</th>
          <td class="tn">TNR={{"%.2f"%TNR}}</td>
          <td class="fp">FPR={{"%.2f"%FPR}}</td>
          <td>{{"%.2f"%(TNR+FPR)}}</td>
        </tr>
        <tr>
          <th>Total</th>
          <td>{{"%.2f"%(TPR+TNR)}}</td>
          <td>{{"%.2f"%(FNR+FPR)}}</td>
          <td>{{"%.2f"%(TPR+FNR+TNR+FPR)}}</td>
        </tr>
      </tbody>
    </table>

    <div class="help" id="proportions_help">
      <ul>
        <li>TPR = True Positive Rate (TP/P)</li>
        <li>FNR = False Negative Rate (FN/P)</li>
        <li>FPR = False Positive Rate (FP/N)</li>
        <li>TNR = True Negative Rate (TN/N)</li>
      </ul>
    </div>
    
  </div>

  <div class="box">

    <h2 class="heading">
      Derived measures
      <button onclick="toggle('derived_help')">?</button>
    </h2>

    <table>
      <tr>
        <th><b>Positive Predictive Value</b> <span class="footnote">(= PPV = TP/(TP+FP))</span></th>
        <td>{{"%.5f"%PPV}}</td>
      </tr>
      <tr>
        <th><b>Negative Preditive Value</b> <span class="footnote">(= NPV = TN/(TN+FN))</span></th>
        <td>{{"%.5f"%NPV}}</td>
      </tr>
      <tr>
        <th><b>Prevalence</b> <span class="footnote">(=P/A)</span></th>
        <td>{{"%.5f"%prevalence}}</td>
      </tr>
      <tr>
        <th><b>Accuracy</b> <span class="footnote">(=T/A)</span></th>
        <td>{{"%.5f"%accuracy}}</td>
      </tr>
    </table>

    <div class="help" id="derived_help">
      <ul>
        <li>PPV = Proportion of those classified positive which actually were positive</li>
        <li>NPV = Proportion of those classified negative which actually were negative</li>
        <li>Prevalence = Proportion of data which was positive</li>
        <li>Accuracy = Proportion of correct classifications</li>
      </ul>
    </div>

  </div>

  <div class="box">

    <h2 class="heading">
      Information Recall (IR) measures
      <button onclick="toggle('ir_help')">?</button>
    </h2>

    <table>
      <tr>
        <th><b>Recall</b> <span class="footnote">(= TPR = sensitivity = TP/P)</span></th>
        <td>{{"%.5f"%TPR}}</td>
      </tr>
      <tr>
        <th><b>Precision</b> <span class="footnote">(= PPV = TP/(TP+FP))</span></th>
        <td>{{"%.5f"%PPV}}</td>
      </tr>
      <tr>
        <th><b>Enrichment</b> <span class="footnote">(= precision/prevalence)</span></th>
        <td>{{"%.5f"%enrichment}}</td>
      </tr>
      <tr>
        <th><b>F-Measure</b> <span class="footnote">(2*recall*precision/(recall+precision))</span></th>
        <td>{{"%.5f"%fmeasure}}</td>
      </tr>
    </table>

    <div class="help" id="ir_help">
      <ul>
        <li>Recall = Proportion of positives classified correctly</li>
        <li>Precision = Proportion of those classified positive which actually were positive</li>
        <li>Enrichment = Precision over prevalence.  This is how many
            times the precision is enriched over a classifier which calls everything positive.</li>
        <li>F-Measure = Combined performance measure of recall and precision</li>
      </ul>
    </div>

  </div>

  <div class="box">

    <h2 class="heading">
      Performance graphs
      <button onclick="toggle('graph_help')">?</button>
    </h2>

    <div class="help" id="graph_help">
      <ul>
        <li>
          The first graph is a score histogram comparing the
          distributions of the background ("negative") articles to
          those of the query ("positive") articles.  When the
          distributions are well-separated, the classifier performance
          is better than when the distributions largely overlap.
        </li>
        <li>
          The second graph is an ROC curve plotting the True Positive
          Rate (or recall) against the False Positive Rate (which is
          1-specificity).  The greater the area under the curve, the
          better the global performance of the classifier.
        </li>
        <li>
          The precision recall curve plots precision against recall.
          Good performance is when the precision remains high at high
          recall.
        </li>
        <li>
          The precision, recall and F-measure against threshold is like the
          previous graph, but shows how tuning the threshold would affect
          the results.  
        </li>
        <li>
          All of the graphs have a vertical line marking the tuned
          threshold.
        </li>
      </ul>
    </div>

    <img src="{{hist_img}}" alt="Score Histogram"/>
    
    <img src="{{roc_img}}" alt="ROC Curve"/>
    
    <img src="{{p_vs_r_img}}" alt="Precision-Recall Curve"/>
    
    <img src="{{pr_vs_score_img}}" alt="Precision and Recall vs Threshold"/>

  </div>
  
</div>

</body>
</html>